{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b9d732aa",
      "metadata": {
        "id": "b9d732aa"
      },
      "source": [
        "# Chapter 0 - 安装与设置"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33885186",
      "metadata": {
        "id": "33885186"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c5d25af0",
      "metadata": {
        "id": "c5d25af0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U crawl4ai\n",
        "!pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e8342717",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8342717",
        "outputId": "0dd70305-01b1-4490-ccf4-7af941c05a27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6.3\n"
          ]
        }
      ],
      "source": [
        "# Check crawl4ai version\n",
        "import crawl4ai\n",
        "print(crawl4ai.__version__.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bfd6121",
      "metadata": {
        "id": "4bfd6121"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f149e07f",
      "metadata": {
        "id": "f149e07f"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!crawl4ai-setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f29d68c",
      "metadata": {
        "id": "7f29d68c"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a0aa2366",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0aa2366",
        "outputId": "eed9a0e6-367f-47ee-99d1-6c3c54b9c5df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Running Crawl4AI health check\u001b[0m\u001b[36m...\u001b[0m\u001b[36m \u001b[0m\n",
            "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m3\u001b[0m\u001b[36m \u001b[0m\n",
            "\u001b[1;36m[\u001b[0m\u001b[36mTEST\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. ℹ Testing crawling capabilities\u001b[0m\u001b[36m...\u001b[0m\u001b[36m \u001b[0m\n",
            "\u001b[1;36m[\u001b[0m\u001b[36mEXPORT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m.. ℹ Exporting media \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mPDF/MHTML/screenshot\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m took \u001b[0m\u001b[1;36m0.\u001b[0m\u001b[36m89s \u001b[0m\n",
            "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                               \u001b[0m\n",
            "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m4.\u001b[0m\u001b[32m88s \u001b[0m\n",
            "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                               \u001b[0m\n",
            "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;33m0.\u001b[0m\u001b[33m02\u001b[0m\u001b[32ms \u001b[0m\n",
            "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://crawl4ai.com\u001b[0m\u001b[32m                                               \u001b[0m\n",
            "\u001b[32m| \u001b[0m\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m4.\u001b[0m\u001b[32m90s \u001b[0m\n",
            "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● ✅ Crawling test passed! \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!crawl4ai-doctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d48b4490",
      "metadata": {
        "id": "d48b4490"
      },
      "outputs": [],
      "source": [
        "import asyncio # 导入Python的异步编程标准库\n",
        "import nest_asyncio # 导入嵌套异步事件循环支持库\n",
        "nest_asyncio.apply() # 允许在Jupyter中使用异步操作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f3605cd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "f3605cd4",
        "outputId": "bbaca3bd-e508-4f96-a52b-33bf5d867d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Example Domain\n"
          ]
        }
      ],
      "source": [
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def test_browser():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless = True)\n",
        "        page = await browser.new_page()\n",
        "        await page.goto('https://example.com')\n",
        "        print(f'Title: {await page.title()}')\n",
        "        await browser.close()\n",
        "\n",
        "asyncio.run(test_browser())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a030c8e0",
      "metadata": {
        "id": "a030c8e0"
      },
      "source": [
        "## *Markdown Output Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b7a9c90",
      "metadata": {
        "id": "3b7a9c90"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "OUTPUT_PATH = 'outputs/markdown/'\n",
        "\n",
        "def output_md(base_filename, md_str):\n",
        "    # 创建输出目录\n",
        "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "    # 生成带长度的文件名\n",
        "    length = len(md_str)\n",
        "    name, ext = os.path.splitext(base_filename)\n",
        "    filename = f\"{name}({length}){ext}\"\n",
        "\n",
        "    # 完整路径\n",
        "    full_path = os.path.join(OUTPUT_PATH, filename)\n",
        "\n",
        "    with open(full_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(md_str)\n",
        "\n",
        "    print(f\"已保存到: {full_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d066ce",
      "metadata": {
        "id": "c7d066ce"
      },
      "source": [
        "# Chapter 1 - 基础形态"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "101673bf",
      "metadata": {
        "id": "101673bf"
      },
      "source": [
        "## 1.1 - Basic Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7fb95a4a",
      "metadata": {
        "id": "7fb95a4a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m3\u001b[0m\u001b[36m \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://www.anthropic.com/news/agent-capabilities-api</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                |</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">49s </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://www.anthropic.com/news/agent-capabilities-api\u001b[0m\u001b[32m                                                |\u001b[0m\n",
              "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m49s \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://www.anthropic.com/news/agent-capabilities-api</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                |</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">02s </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://www.anthropic.com/news/agent-capabilities-api\u001b[0m\u001b[32m                                                |\u001b[0m\n",
              "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m02s \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://www.anthropic.com/news/agent-capabilities-api</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                |</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">52s </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://www.anthropic.com/news/agent-capabilities-api\u001b[0m\u001b[32m                                                |\u001b[0m\n",
              "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m52s \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Markdown length: 11049\n",
            "[Skip to main content](https://www.anthropic.com/news/agent-capabilities-api#main-content)[Skip to footer](https://www.anthropic.com/news/agent-capabilities-api#footer)\n",
            "[](https://www.anthropic.com/)\n",
            "  * Claude\n",
            "  * API\n",
            "  * Solutions\n",
            "  * Research\n",
            "  * Commitments\n",
            "  * Learn\n",
            "[News](https://www.anthropic\n",
            "已保存到: outputs/markdown/1_1_Basic(11049).md\n"
          ]
        }
      ],
      "source": [
        "import asyncio  # 异步编程库\n",
        "from crawl4ai import AsyncWebCrawler  # 网页抓取工具\n",
        "\n",
        "# 异步抓取网页内容\n",
        "async def main(output_filename):\n",
        "    # 创建爬虫对象，自动管理资源(确保爬虫使用完后会自动关闭，释放资源)\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        # 访问指定网址并等待响应(await 关键字表示等待这个操作完成后再继续执行下面的代码)\n",
        "        result = await crawler.arun(\"https://www.anthropic.com/news/agent-capabilities-api\")\n",
        "\n",
        "        # 打印抓取结果\n",
        "        print(\"Markdown length:\", len(result.markdown))\n",
        "        print(result.markdown[:300])\n",
        "\n",
        "        # 保存到.md文件\n",
        "        output_md(output_filename, result.markdown)\n",
        "\n",
        "# 启动异步程序\n",
        "asyncio.run(main('1_1_Basic.md'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95ca0c1b",
      "metadata": {
        "id": "95ca0c1b"
      },
      "source": [
        "# Chapter 2 - 进阶形态"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "805bf7ea",
      "metadata": {
        "id": "805bf7ea"
      },
      "source": [
        "## 2.1 - Setting with BrowerConfig（浏览器配置）"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbdee382",
      "metadata": {
        "id": "dbdee382"
      },
      "source": [
        "BrowserConfig - 控制浏览器本身的行为和启动方式\n",
        "- headless: 是否以无头模式运行, 还是显示完整界面\n",
        "- user_agent: 设置用户代理来模拟不同浏览器\n",
        "- proxy_config: 配置代理服务器等浏览器级别的设置\n",
        "- text_mode: 禁用图片加载，只抓取文本内容"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "244e2ec3",
      "metadata": {
        "id": "244e2ec3"
      },
      "outputs": [],
      "source": [
        "import asyncio  # 异步编程库\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig\n",
        "# AsyncWebCrawler: 异步网页爬虫\n",
        "# BrowserConfig: 浏览器配置\n",
        "# CrawlerRunConfig: 爬虫运行配置\n",
        "# CacheMode: 缓存模式控制\n",
        "\n",
        "# 异步主函数，执行网页爬取任务\n",
        "async def main(output_filename):\n",
        "   # 配置浏览器参数\n",
        "   browser_config = BrowserConfig(\n",
        "       headless = True,  # 无头模式，不显示浏览器窗口\n",
        "       viewport_width = 1280,   # 窗口宽度\n",
        "       viewport_height = 720,   # 窗口高度\n",
        "       user_agent = 'Chrome/114.0.0.0',  # 浏览器标识\n",
        "       text_mode = True, #禁用图片加载，可能会加速仅文本的爬取\n",
        "   )\n",
        "\n",
        "   # 创建异步网页爬虫，自动管理资源\n",
        "   async with AsyncWebCrawler(config = browser_config) as crawler:\n",
        "       # 执行网页爬取\n",
        "        result = await crawler.arun(\n",
        "            url = \"https://www.anthropic.com/news/agent-capabilities-api\",  # 目标网址\n",
        "        )\n",
        "\n",
        "        # 显示爬取结果\n",
        "        print(\"Markdown length:\", len(result.markdown))  # 内容长度\n",
        "        print(result.markdown[:300])  # 前300字符预览\n",
        "\n",
        "        output_md(output_filename, result.markdown)\n",
        "\n",
        "# 启动异步程序\n",
        "asyncio.run(main('2_1_BrowserConfig.md'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7ee5406",
      "metadata": {
        "id": "b7ee5406"
      },
      "source": [
        "## 2.2.0 - Setting with CrawlerRunConfig (爬虫运行配置)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3bc82f6",
      "metadata": {
        "id": "c3bc82f6"
      },
      "source": [
        "CrawlerRunConfig - 控制每次具体爬取任务的执行方式\n",
        "- word_count_threshold: 过滤掉过短的内容，比如导航菜单、按钮文字、简短标签\n",
        "- extraction_strategy: 自定义抓取内容，需要定义json的schema\n",
        "- cache_mode: 缓存策略, 是否使用缓存\n",
        "- js_code: 模拟用户点击[Load More]等按钮\n",
        "- screenshot: 在页面完全加载后自动截取网页截图\n",
        "- pdf: 将整个网页转换为PDF文档\n",
        "- [重要] markdown_generator: 默认DefaultMarkdownGenerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b62aa07",
      "metadata": {
        "id": "3b62aa07"
      },
      "outputs": [],
      "source": [
        "import asyncio  # 异步编程库\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n",
        "# AsyncWebCrawler: 异步网页爬虫\n",
        "# BrowserConfig: 浏览器配置\n",
        "# CrawlerRunConfig: 爬虫运行配置\n",
        "# CacheMode: 缓存模式控制\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "# 异步主函数，执行网页爬取任务\n",
        "async def main(output_filename):\n",
        "   # 配置浏览器参数\n",
        "   browser_config = BrowserConfig(\n",
        "       headless = True,  # 无头模式，不显示浏览器窗口\n",
        "       viewport_width = 1280,   # 窗口宽度\n",
        "       viewport_height = 720,   # 窗口高度\n",
        "       user_agent = 'Chrome/114.0.0.0',  # 浏览器标识\n",
        "       text_mode = True, #禁用图片加载，可能会加速仅文本的爬取\n",
        "   )\n",
        "\n",
        "   # 配置爬虫运行参数\n",
        "   run_config = CrawlerRunConfig(\n",
        "       cache_mode = CacheMode.DISABLED,  # 禁用缓存，获取最新内容\n",
        "       markdown_generator = DefaultMarkdownGenerator(),\n",
        "   )\n",
        "\n",
        "   # 创建异步网页爬虫，自动管理资源\n",
        "   async with AsyncWebCrawler(config = browser_config) as crawler:\n",
        "       # 执行网页爬取\n",
        "        result = await crawler.arun(\n",
        "            url = \"https://www.anthropic.com/news/agent-capabilities-api\",  # 目标网址\n",
        "            config = run_config,  # 运行配置\n",
        "        )\n",
        "\n",
        "        # 显示爬取结果\n",
        "        print(\"Markdown length:\", len(result.markdown))  # 内容长度\n",
        "        print(result.markdown[:300])  # 前300字符预览\n",
        "\n",
        "        output_md(output_filename, result.markdown)\n",
        "\n",
        "# 启动异步程序\n",
        "asyncio.run(main('2_2_0_RunConfig.md'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18e96216",
      "metadata": {
        "id": "18e96216"
      },
      "source": [
        "### 2.2.1 + Content Filter: PruningContentFilter例"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e143b8b",
      "metadata": {
        "id": "6e143b8b"
      },
      "source": [
        "- **markdown_generator**: 核心功能，从网页生成干净、结构化的Markdown\n",
        "    - DefaultMarkdownGenerator(默认且唯一)\n",
        "        - 参数1: Content Filters\n",
        "            - BM25ContentFilter  关键词过滤器\n",
        "            - PruningContentFilter 内容精简过滤器\n",
        "            - LLMContentFilter AI过滤器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0e7e438",
      "metadata": {
        "id": "f0e7e438"
      },
      "outputs": [],
      "source": [
        "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n",
        "from crawl4ai.content_filter_strategy import PruningContentFilter\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "async def main(output_filename):\n",
        "    # 浏览器配置\n",
        "    browser_config = BrowserConfig(headless = True, # 无头模式\n",
        "                                viewport_width = 1280,  # 窗口宽度\n",
        "                                viewport_height = 720,  # 窗口高度\n",
        "                                user_agent = 'Chrome/114.0.0.0', # 浏览器标识\n",
        "                                text_mode = True,\n",
        "                                 )\n",
        "\n",
        "    # 爬虫运行配置\n",
        "    run_config = CrawlerRunConfig(\n",
        "    cache_mode = CacheMode.DISABLED,  # 禁用缓存\n",
        "    markdown_generator = DefaultMarkdownGenerator(\n",
        "        content_filter = PruningContentFilter(\n",
        "            # min_word_threshold = 10, # 丢弃少于N个单词的块，因为它们可能太短或无用(不建议)\n",
        "            threshold = 0.76,  # fixded: 固定阈值 / dynamic: 初始阈值\n",
        "            threshold_type = \"fixed\", # 固定\n",
        "            # threshold_type = \"dynamic\", # 变动\n",
        "        )),\n",
        "    )\n",
        "\n",
        "    # 创建爬虫并执行\n",
        "    async with AsyncWebCrawler(config = browser_config) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url = \"https://www.anthropic.com/news/agent-capabilities-api\",  # 目标网址\n",
        "            config = run_config,  # 运行配置\n",
        "        )\n",
        "\n",
        "        # 保存原始内容\n",
        "        print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n",
        "        output_md(output_filename.replace('.md', '_raw.md'), result.markdown.raw_markdown)\n",
        "\n",
        "        # 保存过滤后内容\n",
        "        print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\n",
        "        output_md(output_filename.replace('.md', '_fit.md'), result.markdown.fit_markdown)\n",
        "\n",
        "asyncio.run(main('2_2_1_RunConfig_ContentFilterPruning.md'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "155724a4",
      "metadata": {
        "id": "155724a4"
      },
      "source": [
        "### 2.2.2 + Options"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03792447",
      "metadata": {
        "id": "03792447"
      },
      "source": [
        "- **markdown_generator**: 核心功能，从网页生成干净、结构化的Markdown\n",
        "    - DefaultMarkdownGenerator(默认且唯一)\n",
        "        - 参数1: Content Filters\n",
        "            - BM25ContentFilter  关键词过滤器\n",
        "            - PruningContentFilter 内容精简过滤器\n",
        "            - LLMContentFilter AI过滤器\n",
        "        - 参数2: Options\n",
        "            - ignore_links (bool): 是否在最终markdown中移除所有超链接\n",
        "            - ignore_images (bool): 移除所有 [[image]]() 图片引用\n",
        "            - escape_html (bool): 将HTML实体转换为文本（默认通常为 True）\n",
        "            - body_width (int): 在N个字符处换行。0 或 None 表示不换行\n",
        "            - skip_internal_links (bool): 如果为 True，忽略 #localAnchors 或引用同一页面的内部链接\n",
        "            - include_sup_sub (bool): 尝试以更易读的方式处理 <sup> / <sub> 标签"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "789140de",
      "metadata": {
        "id": "789140de"
      },
      "outputs": [],
      "source": [
        "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n",
        "from crawl4ai.content_filter_strategy import PruningContentFilter\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "async def main(output_filename):\n",
        "    # 浏览器配置\n",
        "    browser_config = BrowserConfig(headless = True, # 无头模式\n",
        "                                viewport_width = 1280,  # 窗口宽度\n",
        "                                viewport_height = 720,  # 窗口高度\n",
        "                                user_agent = 'Chrome/114.0.0.0', # 浏览器标识\n",
        "                                text_mode = True,\n",
        "                                 )\n",
        "\n",
        "    # 爬虫运行配置\n",
        "    run_config = CrawlerRunConfig(\n",
        "    cache_mode = CacheMode.DISABLED,  # 禁用缓存\n",
        "    markdown_generator = DefaultMarkdownGenerator(\n",
        "        content_filter = PruningContentFilter(\n",
        "            # min_word_threshold = 10, # 丢弃少于N个单词的块，因为它们可能太短或无用(不建议)\n",
        "            threshold = 0.76,  # fixded: 固定阈值 / dynamic: 初始阈值\n",
        "            # threshold_type = \"fixed\", # 固定\n",
        "            threshold_type = \"dynamic\", # 变动\n",
        "        ),\n",
        "        options = {\n",
        "            \"ignore_links\": True,\n",
        "            \"ignore_images\": True,\n",
        "            })\n",
        "    )\n",
        "\n",
        "    # 创建爬虫并执行\n",
        "    async with AsyncWebCrawler(config=browser_config) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url = \"https://www.anthropic.com/news/agent-capabilities-api\",  # 目标网址\n",
        "            config = run_config,  # 运行配置\n",
        "        )\n",
        "\n",
        "        # 保存原始内容\n",
        "        print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n",
        "        output_md(output_filename.replace('.md', '_raw.md'), result.markdown.raw_markdown)\n",
        "\n",
        "        # 保存过滤后内容\n",
        "        print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\n",
        "        output_md(output_filename.replace('.md', '_fit.md'), result.markdown.fit_markdown)\n",
        "\n",
        "asyncio.run(main('2_2_2_RunConfig_ContentFilterPruning_Options.md'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "chihlee1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
