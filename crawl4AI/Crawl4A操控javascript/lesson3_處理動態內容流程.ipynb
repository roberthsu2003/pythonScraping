{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e760d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.1 Load More Example (Hacker News “More” Link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f02f481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m3\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TargetClosedError",
     "evalue": "BrowserContext.close: Target page, context or browser has been closed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTargetClosedError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mItems after load-more:\u001b[39m\u001b[33m\"\u001b[39m, total_items)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#Step 1: Load initial Hacker News page\u001b[39;00m\n\u001b[32m     11\u001b[39m config = CrawlerRunConfig(\n\u001b[32m     12\u001b[39m     wait_for=\u001b[33m\"\u001b[39m\u001b[33mcss:.athing:nth-child(30)\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Wait for 30 items\u001b[39;00m\n\u001b[32m     13\u001b[39m     session_id=\u001b[33m\"\u001b[39m\u001b[33mhn_session\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     cache_mode=CacheMode.BYPASS\n\u001b[32m     15\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncWebCrawler(config=base_browser) \u001b[38;5;28;01mas\u001b[39;00m crawler:\n\u001b[32m     17\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m crawler.arun(\n\u001b[32m     18\u001b[39m         url=\u001b[33m\"\u001b[39m\u001b[33mhttps://news.ycombinator.com\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m         config=config\n\u001b[32m     20\u001b[39m     )\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitial items loaded.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/web_crawler/lib/python3.11/site-packages/crawl4ai/async_webcrawler.py:194\u001b[39m, in \u001b[36mAsyncWebCrawler.__aexit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aexit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/web_crawler/lib/python3.11/site-packages/crawl4ai/async_webcrawler.py:188\u001b[39m, in \u001b[36mAsyncWebCrawler.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    180\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m    Close the crawler explicitly without using context manager.\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[33;03m    This should be called when you're done with the crawler if you used start().\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m \u001b[33;03m    2. Close any open pages and contexts\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.crawler_strategy.\u001b[34m__aexit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/web_crawler/lib/python3.11/site-packages/crawl4ai/async_crawler_strategy.py:115\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.__aexit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aexit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/web_crawler/lib/python3.11/site-packages/crawl4ai/async_crawler_strategy.py:132\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    Close the browser and clean up resources.\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.browser_manager.close()\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# Explicitly reset the static Playwright instance\u001b[39;00m\n\u001b[32m    134\u001b[39m     BrowserManager._playwright_instance = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/web_crawler/lib/python3.11/site-packages/crawl4ai/browser_manager.py:1025\u001b[39m, in \u001b[36mBrowserManager.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m session_ids = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.sessions.keys())\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m session_id \u001b[38;5;129;01min\u001b[39;00m session_ids:\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.kill_session(session_id)\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# Now close all contexts we created. This reclaims memory from ephemeral contexts.\u001b[39;00m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ctx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.contexts_by_config.values():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/web_crawler/lib/python3.11/site-packages/crawl4ai/browser_manager.py:1001\u001b[39m, in \u001b[36mBrowserManager.kill_session\u001b[39m\u001b[34m(self, session_id)\u001b[39m\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m page.close()\n\u001b[32m   1000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_managed_browser:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m context.close()\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sessions[session_id]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/web_crawler/lib/python3.11/site-packages/playwright/async_api/_generated.py:13448\u001b[39m, in \u001b[36mBrowserContext.close\u001b[39m\u001b[34m(self, reason)\u001b[39m\n\u001b[32m  13435\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, reason: typing.Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m  13436\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"BrowserContext.close\u001b[39;00m\n\u001b[32m  13437\u001b[39m \n\u001b[32m  13438\u001b[39m \u001b[33;03m    Closes the browser context. All the pages that belong to the browser context will be closed.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m  13445\u001b[39m \u001b[33;03m        The reason to be reported to the operations interrupted by the context closure.\u001b[39;00m\n\u001b[32m  13446\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m13448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping.from_maybe_impl(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._impl_obj.close(reason=reason))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/web_crawler/lib/python3.11/site-packages/playwright/_impl/_browser_context.py:599\u001b[39m, in \u001b[36mBrowserContext.close\u001b[39m\u001b[34m(self, reason)\u001b[39m\n\u001b[32m    596\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m har.delete()\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._channel._connection.wrap_api_call(_inner_close, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._channel.send(\u001b[33m\"\u001b[39m\u001b[33mclose\u001b[39m\u001b[33m\"\u001b[39m, {\u001b[33m\"\u001b[39m\u001b[33mreason\u001b[39m\u001b[33m\"\u001b[39m: reason})\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._closed_future\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/web_crawler/lib/python3.11/site-packages/playwright/_impl/_connection.py:61\u001b[39m, in \u001b[36mChannel.send\u001b[39m\u001b[34m(self, method, params)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, params: Dict = \u001b[38;5;28;01mNone\u001b[39;00m) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.wrap_api_call(\n\u001b[32m     62\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._inner_send(method, params, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_internal_type,\n\u001b[32m     64\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/web_crawler/lib/python3.11/site-packages/playwright/_impl/_connection.py:528\u001b[39m, in \u001b[36mConnection.wrap_api_call\u001b[39m\u001b[34m(self, cb, is_internal)\u001b[39m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m cb()\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[33m'\u001b[39m\u001b[33mapiName\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    530\u001b[39m     \u001b[38;5;28mself\u001b[39m._api_zone.set(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mTargetClosedError\u001b[39m: BrowserContext.close: Target page, context or browser has been closed"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig,CacheMode,BrowserConfig\n",
    "\n",
    "async def main():\n",
    "    base_browser = BrowserConfig(\n",
    "        browser_type=\"chromium\",\n",
    "        headless=False\n",
    "    )\n",
    "    #Step 1: Load initial Hacker News page\n",
    "    \n",
    "    config = CrawlerRunConfig(\n",
    "        wait_for=\"css:.athing:nth-child(30)\",  # Wait for 30 items\n",
    "        session_id=\"hn_session\",\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "    async with AsyncWebCrawler(config=base_browser) as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://news.ycombinator.com\",\n",
    "            config=config\n",
    "        )\n",
    "        print(\"Initial items loaded.\")\n",
    "\n",
    "        # Step 2: Let's scroll and click the \"More\" link\n",
    "        load_more_js = [\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);\",\n",
    "            # The \"More\" link at page bottom\n",
    "            \"document.querySelector('a.morelink')?.click();\"  \n",
    "        ]\n",
    "\n",
    "        next_page_conf = CrawlerRunConfig(\n",
    "            js_code=load_more_js,\n",
    "            wait_for=\"\"\"js:() => {\n",
    "                return document.querySelectorAll('.athing').length > 30;\n",
    "            }\"\"\",\n",
    "            # Mark that we do not re-navigate, but run JS in the same session:            \n",
    "            cache_mode=CacheMode.BYPASS\n",
    "        )\n",
    "\n",
    "        # Re-use the same crawler session\n",
    "        result2 = await crawler.arun(\n",
    "            url=\"https://news.ycombinator.com\",  # same URL but continuing session\n",
    "            config=next_page_conf\n",
    "        )\n",
    "        total_items = result2.cleaned_html.count(\"athing\")\n",
    "        print(\"Items after load-more:\", total_items)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
